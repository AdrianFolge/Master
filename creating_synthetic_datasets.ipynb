{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "import pandas as pd\n",
    "import re\n",
    "import re\n",
    "import uuid\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('../data/', glob=\"**/*.pdf\", show_progress=True, loader_cls=UnstructuredFileLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "databases = {}\n",
    "for doc in documents:\n",
    "    source = doc.metadata['source']\n",
    "    match = re.search(r'\\/([A-Za-z_]+)\\.pdf', source)\n",
    "    if match:\n",
    "        municipality_name = match.group(1)\n",
    "    docs = text_splitter.split_documents([doc])\n",
    "    databases[municipality_name] = docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_first_item(dictionary):\n",
    "    if not dictionary:\n",
    "        return {}\n",
    "    first_key = next(iter(dictionary))\n",
    "    return {first_key: dictionary[first_key]}\n",
    "filtered_dict = keep_first_item(databases)\n",
    "print(filtered_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_specific_instances(dictionary, indices):\n",
    "    result = {}\n",
    "    for key, value in dictionary.items():\n",
    "        if isinstance(value, list):\n",
    "            instances = [value[i] for i in indices if 0 <= i < len(value)]\n",
    "            result[key] = instances\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result\n",
    "\n",
    "indices_to_keep = [10, 11, 12]\n",
    "filtered_dict = keep_specific_instances(filtered_dict, indices_to_keep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(\n",
    "    corpus,\n",
    "    num_questions_per_chunk=1,\n",
    "    prompt_template=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Automatisk generer hypotetiske spørsmål som kan besvares med dokumentet i korpuset.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model='gpt-4')\n",
    "\n",
    "    prompt_template = prompt_template or \"\"\"\\\n",
    "    Kontekstinformasjonen er nedenfor.\n",
    "\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "\n",
    "    Gitt kontekstinformasjonen og ingen tidligere kunnskap.\n",
    "    Generer bare spørsmål basert på forespørselen nedenfor.\n",
    "    Du er en innbygger som har spørsmål angående planbestemmelser og planreguleringer i kommunen du bor i. Oppgaven din er både å sette opp {num_questions_per_chunk} spørsmål som en typisk innbygger kan lure på fra dokumentene som er gitt, men også svare på dette spørsmålet som en ekspert der svaret ditt inneholder den faktiske informasjonen uten noen henvisning til hvor i dokumentet svaret ligger.\n",
    "    Spørsmålene bør være varierte i naturen på tvers av dokumentet. Begrens spørsmålene og svarene til den kontekstinformasjonen som er gitt.\n",
    "\n",
    "    \"\"\"    \n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    for municipality_name, text_chunks in tqdm(corpus.items()):\n",
    "        for chunk in text_chunks:\n",
    "            query = prompt_template.format(context_str=chunk, num_questions_per_chunk=num_questions_per_chunk)\n",
    "            response = llm.complete(query)\n",
    "\n",
    "            result = str(response).strip().split(\"\\n\")\n",
    "            questions = [\n",
    "                re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "            ]\n",
    "            questions = [question for question in questions if len(question) > 0]\n",
    "            for question in questions:\n",
    "                question_id = str(uuid.uuid4())\n",
    "                queries[question_id] = question\n",
    "                relevant_docs[question_id] = [municipality_name]\n",
    "    return queries, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries, train_relevant_docs = generate_queries(\n",
    "    filtered_dict,\n",
    "    num_questions_per_chunk=1,\n",
    "    prompt_template=None,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = list(train_queries.items())\n",
    "\n",
    "keys = []\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "for i in range(0, len(data_list), 2):\n",
    "    key = data_list[i][0]\n",
    "    question = data_list[i][1].split(': ')[1]\n",
    "    answer = data_list[i+1][1].split(': ')[1]\n",
    "    keys.append(key)\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'nøkkel': keys,\n",
    "    'spørsmål': questions,\n",
    "    'svar': answers\n",
    "})\n",
    "\n",
    "df['kommunenavn'] = [train_relevant_docs[key][0] for key in df['nøkkel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_without_key = df.drop(columns=['nøkkel'])\n",
    "df_without_key.to_csv('Synthetic_data_path', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MASTER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
