{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "import os\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from datasets import load_dataset\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('data/', glob=\"**/*.pdf\", show_progress=True, loader_cls=UnstructuredFileLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = SentenceTransformerEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Qdrant.from_documents(\n",
    "    docs,\n",
    "    embeddings,\n",
    "    location=\":memory:\",  # Local mode with in-memory storage only\n",
    "    collection_name=\"my_documents\",\n",
    ")\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"search_state_of_union\",\n",
    "    \"Searches and returns excerpts from the 2022 State of the Union.\",\n",
    ")\n",
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "\n",
    "agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files=r'/Users/adrianfolge/Documents/lokal:skole/Master/data/synthetic_data/question_with_answers.csv', split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_answers = []\n",
    "for i in range(50):\n",
    "    question = dataset[\"Question\"][i]\n",
    "    result = agent_executor.invoke(\n",
    "        {\n",
    "            \"input\": f\"{question}\"\n",
    "        }\n",
    "    )\n",
    "    list_of_answers.append(result[\"output\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a pandas DataFrame\n",
    "#df = pd.DataFrame(list_of_answers, columns=['Text'])\n",
    "\n",
    "# Specify the file path\n",
    "#file_path = \"/Users/adrianfolge/Documents/lokal:skole/Master/data/Results/Chroma_with_agents.csv\"\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "#df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tensorflow_hub as hub\n",
    "def embed(input, model):\n",
    "    return model(input)\n",
    "\n",
    "def SAS(preds, refs, model):\n",
    "    similarities = []\n",
    "    embeddings_preds = model.encode(preds)\n",
    "    embeddings_refs = model.encode(refs)\n",
    "    for i in range(len(embeddings_preds)):\n",
    "        similarity = util.pytorch_cos_sim(embeddings_preds[i], embeddings_refs[i])\n",
    "        similarities.append(similarity[0][0].item())\n",
    "    average_similarity_score = sum(similarities) / len(similarities)\n",
    "    return average_similarity_score\n",
    "\n",
    "def evaluate_predictions(references, predictions):\n",
    "    ## SAS encoder score\n",
    "    module_url = \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\"\n",
    "    encoder_model = hub.load(module_url)\n",
    "    \n",
    "    list_of_similarity_scores = []\n",
    "    for i in range(len(predictions)):\n",
    "        similarity_score = 1-distance.cosine(embed([predictions[i]], encoder_model)[0, :],embed([references[i]], encoder_model)[0, :])\n",
    "        list_of_similarity_scores.append(similarity_score)\n",
    "    average_score = sum(list_of_similarity_scores) / len(list_of_similarity_scores)\n",
    "\n",
    "    ## SAS transformer score\n",
    "    transformer_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"Metric\": \"Average SAS transformer Score\",\n",
    "        \"Score\":  SAS(predictions, references, transformer_model)\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = dataset[\"Answer\"]\n",
    "preds = list_of_answers\n",
    "evaluate_predictions(references,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jeg_vet_ikke = [references[0]]\n",
    "i_dont_know = [\"Jeg vet ikke\"]\n",
    "evaluate_predictions(jeg_vet_ikke, i_dont_know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "content_list = []\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Task: Answer Evaluation\n",
    "You are given a reference answer and a predicted answer. Your task is to determine whether the predicted answer matches the reference answer correctly. It does not have to be an exact match, but it should be somewhat the same.\n",
    "- The reference answer is the correct answer.\n",
    "- The predicted answer is the answer generated by a model or provided by a user.\n",
    "Your response should indicate whether the predicted answer is correct or not.\n",
    "Reference answer: {reference}\n",
    "Predicted answer: {prediction}\n",
    "Is the predicted answer correct? [Yes/No]\n",
    "agent_scratchpad: This is the scratchpad where you can store intermediate information.\"\"\",\n",
    "    input_variables=[\"prediction\", \"reference\"]\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "for num in range(50):\n",
    "    score = chain.invoke(\n",
    "        {\n",
    "            \"reference\": references[num],\n",
    "            \"prediction\": list_of_answers[num],\n",
    "        }\n",
    "    )\n",
    "    content_list.append(score.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_yes = content_list.count('Yes')\n",
    "count_no = content_list.count('No')\n",
    "\n",
    "# Displaying the counts\n",
    "print(\"Number of 'Yes':\", count_yes)\n",
    "print(\"Number of 'No':\", count_no)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MASTER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
