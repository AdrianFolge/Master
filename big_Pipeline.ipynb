{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing libraries\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings, OpenAIEmbeddings\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS, Qdrant, Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "import tensorflow_hub as hub\n",
    "from scipy.spatial import distance\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "## Some evaluation functions\n",
    "def embed(input, model):\n",
    "    return model(input)\n",
    "\n",
    "def SAS(preds, refs, model):\n",
    "    similarities = []\n",
    "    embeddings_preds = model.encode(preds)\n",
    "    embeddings_refs = model.encode(refs)\n",
    "    for i in range(len(embeddings_preds)):\n",
    "        similarity = util.pytorch_cos_sim(embeddings_preds[i], embeddings_refs[i])\n",
    "        similarities.append(similarity[0][0].item())\n",
    "    average_similarity_score = sum(similarities) / len(similarities)\n",
    "    return average_similarity_score\n",
    "\n",
    "def evaluate_predictions(references, predictions, text_splitter_name,embedding_name,db_name):\n",
    "\n",
    "    bertscore = load(\"bertscore\")\n",
    "    bleu = evaluate.load('bleu')\n",
    "    rouge = evaluate.load('rouge')\n",
    "\n",
    "    references = references[\"Answer\"]\n",
    "    predictions = predictions[\"train\"][\"Text\"]\n",
    "\n",
    "    bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"nb\")\n",
    "    bleu_score = bleu.compute(predictions=predictions, references=references, max_order=2)\n",
    "    rouge_score = rouge.compute(predictions=predictions, references=references)\n",
    "\n",
    "    avg_precision = sum(bert_score['precision']) / len(bert_score['precision'])\n",
    "    avg_recall = sum(bert_score['recall']) / len(bert_score['recall'])\n",
    "    avg_f1 = sum(bert_score['f1']) / len(bert_score['f1'])\n",
    "\n",
    "    ## SAS encoder score\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "    encoder_model = hub.load(module_url)\n",
    "    \n",
    "    list_of_similarity_scores = []\n",
    "    for i in range(len(predictions)):\n",
    "        similarity_score = 1-distance.cosine(embed([predictions[i]], encoder_model)[0, :],embed([references[i]], encoder_model)[0, :])\n",
    "        list_of_similarity_scores.append(similarity_score)\n",
    "    average_score = sum(list_of_similarity_scores) / len(list_of_similarity_scores)\n",
    "\n",
    "    ## SAS transformer score\n",
    "    transformer_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"Name_of_model\": [text_splitter_name, embedding_name, db_name],\n",
    "        \"Metric\": [\"BLEU Score\", \"ROUGE Score\", \"Average Precision\", \"Average Recall\", \"Average F1 Score\", \"Average SAS encoder Score\", \"Average SAS transformer Score\"],\n",
    "        \"Score\": [bleu_score, rouge_score, avg_precision, avg_recall, avg_f1, average_score, SAS(predictions, references)]\n",
    "    }\n",
    "    return data\n",
    "\n",
    "\n",
    "counter = 0\n",
    "all_data = []\n",
    "##Input tokenizer and output model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"RuterNorway/Llama-2-13b-chat-norwegian\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"RuterNorway/Llama-2-13b-chat-norwegian\")\n",
    "\n",
    "##Loading dataset\n",
    "dataset = load_dataset('csv', data_files=r'/Users/adrianfolge/Documents/lokal:skole/Master/data/synthetic_data/question_with_answers.csv', split=\"train[:10]\")\n",
    "\n",
    "## Open AI key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "## Defining document loader\n",
    "loader = DirectoryLoader('data/', glob=\"**/*.pdf\", show_progress=True, loader_cls=UnstructuredFileLoader)\n",
    "documents = loader.load()\n",
    "\n",
    "## Split the documents to chunks of text\n",
    "Character_Text_Splitter_100_20 = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "Recursive_Text_Splitter_100_20 = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "Character_Text_Splitter_500_50 = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "Recursive_Text_Splitter_500_50 = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "Character_Text_Splitter_1000_100 = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "Recursive_Text_Splitter_1000_100 = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "splitters = [Character_Text_Splitter_100_20,Recursive_Text_Splitter_100_20, Character_Text_Splitter_500_50, Recursive_Text_Splitter_500_50, Character_Text_Splitter_1000_100, Recursive_Text_Splitter_1000_100]\n",
    "for split in splitters:\n",
    "    docs = split.split_documents(documents)\n",
    "\n",
    "    ## Defining embeddings\n",
    "    paraphrase = SentenceTransformerEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2') \n",
    "    e5 = SentenceTransformerEmbeddings(model_name='intfloat/multilingual-e5-large')\n",
    "    OpenAIEmbeddings = OpenAIEmbeddings()\n",
    "    embeddings = [paraphrase, OpenAIEmbeddings, e5]\n",
    "    for embedding in embeddings:\n",
    "        ## Defining vectorstores\n",
    "        qdrant = Qdrant.from_documents(docs, embedding, location=\":memory:\", collection_name=\"my_documents\")\n",
    "        FAISS = FAISS.from_documents(docs, embedding)\n",
    "        Chroma = Chroma.from_documents(docs, embedding)\n",
    "        db = [FAISS, Chroma, qdrant]\n",
    "        for db in db:\n",
    "            print(f\"Starter med {counter}/54\")\n",
    "            answers_from_model = []\n",
    "            for i in range(10):\n",
    "                query = dataset[\"Question\"][i]\n",
    "                found_docs = db.similarity_search(query)\n",
    "                context = found_docs[0].page_content\n",
    "                input = f\"Spørsmål: {query} context: {context}\"\n",
    "                instruction = \"Svar på spørsmålet basert på det som står i 'context'\"\n",
    "                prompt_template=f'''### Instruction: {instruction}\n",
    "                ### Input: {input}\n",
    "                ### Response:\n",
    "                '''\n",
    "                inputs = tokenizer(prompt_template, return_tensors=\"pt\")\n",
    "\n",
    "                out = model.generate(**inputs, max_new_tokens=200)\n",
    "                print(tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "                # Pipeline prompting\n",
    "                pipe = pipeline(\n",
    "                    \"text-generation\",\n",
    "                    model=model,\n",
    "                    do_sample=True,\n",
    "                    tokenizer=tokenizer,\n",
    "                    max_new_tokens=512,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.95,\n",
    "                    repetition_penalty=1.15\n",
    "                )\n",
    "                answers_from_model.append(pipe(prompt_template)[0]['generated_text'][len(prompt_template):])\n",
    "            preds = answers_from_model\n",
    "            text_splitter_name = str(split._chunk_size)+split.__class__.__name__\n",
    "            if hasattr(embedding, 'model_name'):\n",
    "                embedding_name = embedding.model_name\n",
    "            else:\n",
    "                embedding_name = embedding.model\n",
    "            eval = evaluate_predictions(dataset, preds, text_splitter_name,embedding_name,db)\n",
    "            all_data.append(eval)\n",
    "            counter+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MASTER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
