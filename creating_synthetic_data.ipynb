{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This uses OpenAI to create synthetic questions, abstracts and answers based off the contents in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import textract\n",
    "from pathlib import Path\n",
    "import re\n",
    "import uuid\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extract text from a PDF file.\n",
    "\n",
    "    Args:\n",
    "    pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "    list: List of text chunks extracted from the PDF.\n",
    "    \"\"\"\n",
    "    # Open the PDF file\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        # Create a PDF reader object\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        # Initialize an empty list to store text chunks\n",
    "        text_chunks = []\n",
    "        # Iterate through each page in the PDF\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            # Get the page object\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            # Extract text from the page and append it to the list\n",
    "            text_chunks.append(page.extract_text())\n",
    "    return text_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Extract text from all PDF files in a folder.\n",
    "\n",
    "    Args:\n",
    "    folder_path (str): Path to the folder containing PDF files.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are file names and values are extracted text.\n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to store text from each file\n",
    "    text_dict = {}\n",
    "    # Iterate through each file in the folder\n",
    "    for file_path in Path(folder_path).iterdir():\n",
    "        if file_path.suffix.lower() == '.pdf':\n",
    "            # Extract text from the PDF file\n",
    "            text = extract_text_from_pdf(str(file_path))\n",
    "            # Store the extracted text in the dictionary\n",
    "            text_dict[file_path.name] = text\n",
    "    return text_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './pdf'\n",
    "pdf_text_dict = extract_text_from_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(\n",
    "    corpus,\n",
    "    num_questions_per_chunk=1,\n",
    "    prompt_template=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Automatisk generer hypotetiske spørsmål som kunne besvares med dokumentet i korpuset.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "    prompt_template = prompt_template or \"\"\"\\\n",
    "    Kontekstinformasjonen er nedenfor.\n",
    "\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "\n",
    "    Gitt kontekstinformasjonen og ikke tidligere kunnskap.\n",
    "    Generer bare spørsmål basert på forespørselen nedenfor.\n",
    "\n",
    "    Du er en lærer/professor. \n",
    "    Oppgaven din er å sette opp {num_questions_per_chunk} spørsmål for en kommende quiz/eksamen. \n",
    "    Spørsmålene bør være varierte i naturen på tvers av dokumentet. \n",
    "    Begrens spørsmålene til den kontekstinformasjonen som er gitt.\"\n",
    "    \"\"\"    \n",
    "    queries = {}\n",
    "    relevant_docs = {}\n",
    "    for node_id, text in tqdm(corpus.items()):\n",
    "        for chunk in text:\n",
    "            query = prompt_template.format(context_str=chunk, num_questions_per_chunk=num_questions_per_chunk)\n",
    "            response = llm.complete(query)\n",
    "\n",
    "            result = str(response).strip().split(\"\\n\")\n",
    "            questions = [\n",
    "                re.sub(r\"^\\d+[\\).\\s]\", \"\", question).strip() for question in result\n",
    "            ]\n",
    "            questions = [question for question in questions if len(question) > 0]\n",
    "\n",
    "            for question in questions:\n",
    "                question_id = str(uuid.uuid4())\n",
    "                queries[question_id] = question\n",
    "                relevant_docs[question_id] = [node_id]\n",
    "    return queries, relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_queries, train_relevant_docs = generate_queries(pdf_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = {\n",
    "    'Question': train_queries,\n",
    "    'Corpus': pdf_text_dict,\n",
    "    'Abstract': train_relevant_docs,\n",
    "}\n",
    "\n",
    "dataset = train_dataset\n",
    "\n",
    "corpus = dataset['Corpus']\n",
    "queries = dataset['Question']\n",
    "relevant_docs = dataset['Abstract']\n",
    "\n",
    "examples = []\n",
    "for query_id, query in queries.items():\n",
    "    node_id = relevant_docs[query_id][0]\n",
    "    text = corpus[node_id]\n",
    "    example = {\"Question\" : query, \"Abstract\" : text}\n",
    "    examples.append(example)\n",
    "import pandas as pd\n",
    "\n",
    "question_abstract_pair_df = pd.DataFrame(examples)\n",
    "question_abstract_pair_df.to_csv(\"./question_abstract_pair.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(\n",
    "    query,\n",
    "    context,\n",
    "    prompt_template=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Automatisk generer hypotetiske spørsmål som kunne besvares med dokumentasjonen i korpuset.  \n",
    "    \"\"\"\n",
    "    llm = OpenAI(model='gpt-3.5-turbo')\n",
    "\n",
    "    prompt_template = prompt_template or \"\"\"\\\n",
    "    \n",
    "    Kontekstinformasjonen er nedenfor.\n",
    "\n",
    "    ---------------------\n",
    "    {context_str}\n",
    "    ---------------------\n",
    "\n",
    "    Gitt kontekstinformasjonen og ikke tidligere kunnskap, generer bare svar basert på den nedenfor gitte spørringen.\n",
    "\n",
    "    ---------------------\n",
    "    {query_str}\n",
    "    ---------------------\n",
    "\n",
    "    Du er en lærer/professor. Oppgaven din er å svare på spørsmål til en kommende quiz/eksamen. Begrens svarene dine basert på den gitte kontekstinformasjonen. Hvis du ikke vet svaret, svar bare: \"Jeg vet ikke\".\n",
    "    \"\"\"\n",
    "    full_query = prompt_template.format(context_str=context, query_str=query)\n",
    "    response = llm.complete(full_query)\n",
    "\n",
    "    result = str(response).strip().split(\"\\n\")\n",
    "    answers = [\n",
    "            re.sub(r\"^\\d+[\\).\\s]\", \"\", answer).strip() for answer in result\n",
    "        ]\n",
    "    answers = [answer for answer in answers if len(answer) > 0]\n",
    "    return answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example in tqdm(examples[:100]):\n",
    "  example[\"Answer\"] = generate_answer(example[\"Question\"], example[\"Abstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(examples[:52])\n",
    "train_df.to_csv(\"question_with_answers.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
