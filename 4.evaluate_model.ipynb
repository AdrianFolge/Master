{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from evaluate import load\n",
    "import tensorflow_hub as hub\n",
    "from scipy.spatial import distance\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "bleu = evaluate.load('bleu')\n",
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = load_dataset('csv', data_files=r'/Users/adrianfolge/Documents/lokal:skole/Master/data/synthetic_data/question_with_answers.csv', split=\"train[:10]\")\n",
    "predictions = load_dataset('csv', data_files=r'/Users/adrianfolge/Documents/lokal:skole/Master/data/Results/Qdrant_with_agents_OpenAIEmbeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = references[\"Answer\"]\n",
    "predictions = predictions[\"train\"][\"Text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_score = bertscore.compute(predictions=predictions, references=references, lang=\"nb\")\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references, max_order=2)\n",
    "rouge_score = rouge.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_precision = sum(bert_score['precision']) / len(bert_score['precision'])\n",
    "avg_recall = sum(bert_score['recall']) / len(bert_score['recall'])\n",
    "avg_f1 = sum(bert_score['f1']) / len(bert_score['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAS encoder score\n",
    "module_url = \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\"\n",
    "model = hub.load(module_url)\n",
    "#print(\"module %s loaded\" % module_url)\n",
    " \n",
    " \n",
    "def embed(input):\n",
    "    return model(input)\n",
    "list_of_similarity_scores = []\n",
    "for i in range(len(predictions)):\n",
    "    similarity_score = 1-distance.cosine(embed([predictions[i]])[0, :],embed([references[i]])[0, :])\n",
    "    list_of_similarity_scores.append(similarity_score)\n",
    "    print(f'\\nPrediction: {predictions[i]}\\nReference: {references[i]}\\nSimilarity Score = {similarity_score} ')\n",
    "average_score = sum(list_of_similarity_scores) / len(list_of_similarity_scores)\n",
    "print(\"Average similarity score:\", average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAS transformer score\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "def SAS(preds, refs):\n",
    "    similarities = []\n",
    "    embeddings_preds = model.encode(preds)\n",
    "    embeddings_refs = model.encode(refs)\n",
    "    for i in range(len(embeddings_preds)):\n",
    "        similarity = util.pytorch_cos_sim(embeddings_preds[i], embeddings_refs[i])\n",
    "        similarities.append(similarity[0][0].item())\n",
    "    average_similarity_score = sum(similarities) / len(similarities)\n",
    "    return average_similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BLEU SCORES\")\n",
    "print(bleu_score)\n",
    "print(\"ROUGE SCORES\")\n",
    "print(rouge_score)\n",
    "print(\"BERT SCORES\")\n",
    "print(\"Average Precision:\", avg_precision)\n",
    "print(\"Average Recall:\", avg_recall)\n",
    "print(\"Average F1 Score:\", avg_f1)\n",
    "print(\"Average SAS encoder Score:\", average_score)\n",
    "print(\"Average SAS transformer Score:\", SAS(predictions, references))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Metric\": [\"BLEU Score\", \"ROUGE Score\", \"Average Precision\", \"Average Recall\", \"Average F1 Score\", \"Average SAS encoder Score\", \"Average SAS transformer Score\"],\n",
    "    \"Score\": [bleu_score, rouge_score, avg_precision, avg_recall, avg_f1, average_score, SAS(predictions, references)]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Specify the file path\n",
    "file_path = \"/Users/adrianfolge/Documents/lokal:skole/Master/data/Evaluation_scores/Qdrant_agent_OpenAIEmbeddings_evaluation_scores.csv\"\n",
    "\n",
    "# Write DataFrame to CSV\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "print(\"Data has been written to\", file_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
