{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from deep_translator import GoogleTranslator\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the pdf and split the text to smaller chunks \n",
    "loader = DirectoryLoader('data/', glob=\"**/*.pdf\", show_progress=True, loader_cls=UnstructuredFileLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Translating each chunk of text\n",
    "for document in docs:\n",
    "    page_content = document.page_content\n",
    "    translated_content = GoogleTranslator(source='no', target='en').translate(text=page_content)\n",
    "    document.page_content = translated_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If the content is of type None, give it an empty string\n",
    "for index, doc in enumerate(docs):\n",
    "    if isinstance(doc.page_content, type(None)):\n",
    "        docs[index].page_content = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the database\n",
    "db = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = load_dataset('csv', data_files=r'/Users/adrianfolge/Documents/lokal:skole/Master/data/synthetic_data/question_with_answers.csv', split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_answers = []\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Task: Provide an answer\n",
    "You are going to provide an answer to this question: {question}, based off this context: {context}. Give the answer in Norwegian.\n",
    "agent_scratchpad: This is the scratchpad where you can store intermediate information.\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "for num in range(50):\n",
    "    query = references[\"Question\"][num]\n",
    "    translated_query = GoogleTranslator(source='no', target='en').translate(text=query)\n",
    "    found_docs = db.similarity_search(translated_query)\n",
    "    context=\"Context\"\n",
    "    for doc in found_docs:\n",
    "        context+=doc.page_content\n",
    "    answer = chain.invoke(\n",
    "        {\n",
    "            \"question\": translated_query,\n",
    "            \"context\": context,\n",
    "        }\n",
    "    )\n",
    "    translated_answer = GoogleTranslator(source='en', target='no').translate(text=answer.content)\n",
    "    list_of_answers.append(translated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refs = references[\"Answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = []\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Task: Answer Evaluation\n",
    "You are given a reference answer and a predicted answer. Your task is to determine whether the predicted answer matches the reference answer correctly. It does not have to be an exact match, but it should be somewhat the same.\n",
    "- The reference answer is the correct answer.\n",
    "- The predicted answer is the answer generated by a model or provided by a user.\n",
    "Your response should indicate whether the predicted answer is correct or not.\n",
    "Reference answer: {reference}\n",
    "Predicted answer: {prediction}\n",
    "Is the predicted answer correct? [Yes/No]\n",
    "agent_scratchpad: This is the scratchpad where you can store intermediate information.\"\"\",\n",
    "    input_variables=[\"prediction\", \"reference\"]\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "for num in range(50):\n",
    "    score = chain.invoke(\n",
    "        {\n",
    "            \"reference\": refs[num],\n",
    "            \"prediction\": list_of_answers[num],\n",
    "        }\n",
    "    )\n",
    "    content_list.append(score.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_yes = content_list.count('Yes')\n",
    "count_no = content_list.count('No')\n",
    "\n",
    "# Displaying the counts\n",
    "print(\"Number of 'Yes':\", count_yes)\n",
    "print(\"Number of 'No':\", count_no)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MASTER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
