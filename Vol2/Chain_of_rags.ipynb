{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load packages\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from datasets import load_dataset\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "import re\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "import os\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain import hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initalize OS key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load training dataset\n",
    "references = load_dataset('csv', data_files=r'/Users/adrianfolge/Documents/lokal:skole/Master/data/synthetic_data/vol2_questions_and_answers_ytterligere_revidert.csv', split=f\"train[:{instances}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split and chunk the data\n",
    "loader = DirectoryLoader('../data/', glob=\"**/*.pdf\", show_progress=True, loader_cls=UnstructuredFileLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "databases = {}\n",
    "for doc in documents:\n",
    "    source = doc.metadata['source']\n",
    "    match = re.search(r'\\/([A-Za-z_]+)\\.pdf', source)\n",
    "    if match:\n",
    "        municipality_name = match.group(1)\n",
    "    docs = text_splitter.split_documents([doc])\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    db = FAISS.from_documents(docs, embeddings)\n",
    "    databases[municipality_name] = db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use RAG with React few shots at the same time(?)\n",
    "# Get the prompt to use - you can modify this!\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "prompt = PromptTemplate(template=\"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question. Answer in Norwegian\n",
    "\n",
    "Few-shot examples:\n",
    "Question: Hva er datoen for vedtaket av Kommunedelplan for sentrum av bystyret?\n",
    "Final Answer: Datoen for vedtaket av Kommunedelplan for sentrum av bystyret er 26.8.2021.\n",
    "\n",
    "Question: Hvor kan man finne felles bestemmelser i dokumentet om Kristiansund?\n",
    "Final Answer: Felles bestemmelser kan finnes på side 3 i dokumentet om Kristiansund.\n",
    "\n",
    "Question: Hva er hovedtemaet for avsnitt 3.6 i dokumentet?\n",
    "Final Answer: Hovedtemaet for avsnitt 3.6 i dokumentet er offentlige områder.\n",
    "\n",
    "Question: Hva er emnet for kapittel 4 i dokumentet fra Kristiansund?\n",
    "Final Answer: Emnet for kapittel 4 i dokumentet fra Kristiansund er bebyggelse og anlegg.\n",
    "                        \n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad} \n",
    "\"\"\", input_variables=[\"tool_names\", \"tools\", \"input\", \"agent_scratchpad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_answers_react = []\n",
    "for num in range(instances):\n",
    "    answer_and_similar_docs = {}\n",
    "    query = references[\"spørsmål\"][num]\n",
    "    kommunenavn = references[\"kommunenavn\"][num]\n",
    "    db = databases[kommunenavn]\n",
    "    found_docs = db.similarity_search(query)\n",
    "    all_page_contents = []\n",
    "    # Iterate over each document in found_docs\n",
    "    for doc in found_docs:\n",
    "        # Extract the content of the document\n",
    "        content = doc.page_content\n",
    "        # Append the content to the list\n",
    "        all_page_contents.append(content)\n",
    "\n",
    "    # Join all the extracted contents together into one big chunk of text\n",
    "    big_chunk_of_text = '\\n'.join(all_page_contents)\n",
    "    retriever = db.as_retriever()\n",
    "    tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"search_planning_regulations\",\n",
    "        \"Searches and returns excerpts planning regulation documents from different municipalities\",\n",
    "    )\n",
    "    tools = [tool]\n",
    "    agent = create_react_agent(llm, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
    "    answer = agent_executor.invoke({\"input\": {query}})\n",
    "    print(\"########### HER ER ANSWER\")\n",
    "    print(answer)\n",
    "    answer_and_similar_docs[\"svar\"] = answer[\"output\"]\n",
    "    answer_and_similar_docs[\"kontekst\"] = big_chunk_of_text\n",
    "    list_of_answers_react.append(answer_and_similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the RAG with more context\n",
    "list_of_answers_RAG_more_context = []\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Task: Provide an answer\n",
    "You are going to provide an answer to this question: {question}, based off this context: {context}.\n",
    "Keep the answer to no longer than a sentence. Give the answer in Norwegian.\n",
    "agent_scratchpad: This is the scratchpad where you can store intermediate information.\"\"\",\n",
    "    input_variables=[\"question\", \"context\"]\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "for num in range(instances):\n",
    "    answer_and_similar_docs = {}\n",
    "    query = references[\"spørsmål\"][num]\n",
    "    kommunenavn = references[\"kommunenavn\"][num]\n",
    "    db = databases[kommunenavn]\n",
    "    found_docs = db.similarity_search(query)\n",
    "    context=\"Context\"\n",
    "    for doc in found_docs:\n",
    "        context+=doc.page_content\n",
    "    print(context)\n",
    "    answer = chain.invoke(\n",
    "        {\n",
    "            \"question\": query,\n",
    "            \"context\": context,\n",
    "        }\n",
    "    )\n",
    "    answer_and_similar_docs[\"svar\"] = answer.content\n",
    "    answer_and_similar_docs[\"kontekst\"] = context\n",
    "    list_of_answers_RAG_more_context.append(answer_and_similar_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RAG with simple agent\n",
    "prompt = hub.pull(\"hwchase17/openai-tools-agent\")\n",
    "prompt.messages\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "list_of_answers_with_simple_agent = []\n",
    "for i in range(instances):\n",
    "    question = references[\"spørsmål\"][i]\n",
    "    kommunenavn = references[\"kommunenavn\"][i]\n",
    "    db = databases[kommunenavn]\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    tool = create_retriever_tool(\n",
    "        retriever,\n",
    "        \"search_state_of_union\",\n",
    "        \"Searches and returns excerpts from the 2022 State of the Union.\",\n",
    "    )\n",
    "    tools = [tool]\n",
    "\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    \n",
    "    result = agent_executor.invoke(\n",
    "        {\n",
    "            \"input\": f\"{question}\"\n",
    "        }\n",
    "    )\n",
    "    list_of_answers_with_simple_agent.append(result[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the answers and make it resonate.\n",
    "## Use the RAG again for the questions that are wrong\n",
    "## Evaulate again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = []\n",
    "preds_react =list_of_answers_react\n",
    "preds_more_context = list_of_answers_RAG_more_context\n",
    "preds_simple_agent = list_of_answers_with_simple_agent\n",
    "refs = references[\"svar\"]\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Task: Answer Evaluation\n",
    "You are given a question, three prediction answers, and two chunks of context. Your task is to determine if any of the answers correctly answer the question and which predicted answer better answers the question based on the provided context. If none of the answers answer the question correctly, return None. Only answer with Prediction 1 or Prediction 2 or Prediction 3 or None.\n",
    "- Question: {question}\n",
    "- Prediction 1: {prediction_1}\n",
    "- Prediction 2: {prediction_2}\n",
    "- Prediction 3: {prediction_3}\n",
    "- Context 1: {context_1}\n",
    "- Context 2: {context_2}\n",
    "Which predicted answer better answers the question based on the provided context? [Prediction 1/Prediction 2/Prediction 3/None]\n",
    "agent_scratchpad: This is the scratchpad where you can store intermediate information.\"\"\",\n",
    "    input_variables=[\"question\", \"prediction_1\", \"prediction_2\", \"prediction_3\", \"context_1\", \"context_2\"]\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "for num in range(instances):\n",
    "    score = chain.invoke(\n",
    "        {\n",
    "            \"question\": refs[num],  # Assuming refs contains the questions\n",
    "            \"prediction_1\": preds_react[num][\"svar\"],\n",
    "            \"prediction_2\": preds_more_context[num][\"svar\"],\n",
    "            \"prediction_3\": preds_simple_agent[num],\n",
    "            \"context_1\": preds_react[num][\"kontekst\"],  # Assuming context_chunk_1 contains the first context\n",
    "            \"context_2\": preds_more_context[num][\"kontekst\"],  # Assuming context_chunk_2 contains the second context\n",
    "        }\n",
    "    )\n",
    "    content_list.append(score.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Check through if any of the answers from the previous model is NONE. This means that none of the predicted answers were deemed as correct. We therefore try to extract the correct answer again.\n",
    "indexer_for_none = []\n",
    "for num in range(instances):\n",
    "    if (\"None\" in content_list[num]):\n",
    "        indexer_for_none.append(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get answers for the indexes the evaluation model deemed as not answering the question\n",
    "revised_answers = {}\n",
    "preds_react =list_of_answers_react\n",
    "preds_more_context = list_of_answers_RAG_more_context\n",
    "preds_simple_agent = list_of_answers_with_simple_agent\n",
    "refs = references[\"svar\"]\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Task: Answer Evaluation\n",
    "You are given a question and two chunks of context. Your task is to best answer the questions based on the information in the chunks of context. Only use the context to answer the question and give the answer in Norwegian.\n",
    "- Question: {question}\n",
    "- Context 1: {context_1}\n",
    "- Context 2: {context_2}\n",
    "agent_scratchpad: This is the scratchpad where you can store intermediate information.\"\"\",\n",
    "    input_variables=[\"question\", \"context_1\", \"context_2\"]\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "for num in indexer_for_none:\n",
    "    score = chain.invoke(\n",
    "        {\n",
    "            \"question\": refs[num],  # Assuming refs contains the questions\n",
    "            \"context_1\": preds_react[num][\"kontekst\"],  # Assuming context_chunk_1 contains the first context\n",
    "            \"context_2\": preds_more_context[num][\"kontekst\"],  # Assuming context_chunk_2 contains the second context\n",
    "        }\n",
    "    )\n",
    "    revised_answers[num] = score.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endelig_liste_med_svar = []\n",
    "for num in range(instances):\n",
    "    if (\"Prediction 1\" in content_list[num]):\n",
    "        endelig_liste_med_svar.append(list_of_answers_react[num][\"svar\"])\n",
    "    elif (\"Prediction 2\" in content_list[num]):\n",
    "        endelig_liste_med_svar.append(preds_more_context[num][\"svar\"])\n",
    "    elif (\"Prediction 3\" in content_list[num]):\n",
    "        endelig_liste_med_svar.append(preds_simple_agent[num])\n",
    "    else:\n",
    "        endelig_liste_med_svar.append(revised_answers[num])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ja_nei_liste = []\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Task: Answer Evaluation\n",
    "You are given a reference answer and a predicted answer. Your task is to determine whether the predicted answer matches the reference answer correctly. It does not have to be an exact match, but it should be somewhat the same.\n",
    "- The reference answer is the correct answer.\n",
    "- The predicted answer is the answer generated by a model or provided by a user.\n",
    "Your response should indicate whether the predicted answer is correct or not.\n",
    "Reference answer: {reference}\n",
    "Predicted answer: {prediction}\n",
    "Is the predicted answer correct? [Yes/No]\n",
    "agent_scratchpad: This is the scratchpad where you can store intermediate information.\"\"\",\n",
    "    input_variables=[\"prediction\", \"reference\"]\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "for num in range(instances):\n",
    "    score = chain.invoke(\n",
    "        {\n",
    "            \"reference\": refs[num],\n",
    "            \"prediction\": endelig_liste_med_svar[num],\n",
    "        }\n",
    "    )\n",
    "    ja_nei_liste.append(score.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_of_yes = 0\n",
    "\n",
    "# Iterate over content_list\n",
    "for content in ja_nei_liste:\n",
    "    # Check if \"Yes\" is present in the content\n",
    "    if \"Yes\" in content:\n",
    "        # Increment count if \"Yes\" is found\n",
    "        count_of_yes += 1\n",
    "\n",
    "print(\"Count of 'Yes':\", count_of_yes)\n",
    "\n",
    "count_of_no = 0\n",
    "\n",
    "# Iterate over content_list\n",
    "for content in ja_nei_liste:\n",
    "    # Check if \"Yes\" is present in the content\n",
    "    if \"No\" in content:\n",
    "        # Increment count if \"Yes\" is found\n",
    "        count_of_no += 1\n",
    "\n",
    "print(\"Count of 'no':\", count_of_no)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MASTER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
