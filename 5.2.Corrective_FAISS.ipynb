{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader, DirectoryLoader\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "import os\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from datasets import load_dataset\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Dict, TypedDict\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import pprint\n",
    "from langgraph.graph import END, StateGraph\n",
    "from scipy.spatial import distance\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import tensorflow_hub as tensor_hub\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader('data/', glob=\"**/*.pdf\", show_progress=True, loader_cls=UnstructuredFileLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "embeddings = SentenceTransformerEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "db  = FAISS.from_documents(\n",
    "    docs,\n",
    "    embeddings)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        keys: A dictionary where each key is a string.\n",
    "    \"\"\"\n",
    "\n",
    "    keys: Dict[str, any]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Nodes ###\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = retriever.get_relevant_documents(question)\n",
    "    transform_attempts = state_dict.get(\"transform_attempts\", 0)  # Initialize transform_attempts if not present\n",
    "    return {\"keys\": {\"documents\": documents, \"question\": question, \"transform_attempts\": transform_attempts}}\n",
    "\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    transform_attempts = state_dict[\"transform_attempts\"]\n",
    "    \n",
    "\n",
    "\n",
    "    # Prompt\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Post-processing\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    # Chain\n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    # Run\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": question, \"generation\": generation, \"transform_attempts\": transform_attempts}\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK RELEVANCE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    transform_attempts = state_dict[\"transform_attempts\"]\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Du er en vurderer som vurderer relevansen til et hentet dokument for et brukerspørsmål. \\n\n",
    "        Her er det hentede dokumentet: \\n\\n {context} \\n\\n\n",
    "        Her er brukerspørsmålet: {question} \\n\n",
    "        Hvis dokumentet inneholder nøkkelord relatert til brukerspørsmålet, vurder det som relevant. \\n\n",
    "        Det trenger ikke å være en streng test. Målet er å filtrere ut feilaktige hentinger. \\n\n",
    "        Gi en binær score 'ja' eller 'nei' for å indikere om dokumentet er relevant for spørsmålet. \\n\n",
    "        Gi den binære scoren som en JSON med en enkelt nøkkel 'score' og ingen innledning eller forklaring.\"\"\",\n",
    "        input_variables=[\"question\", \"context\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | llm | JsonOutputParser()\n",
    "\n",
    "    # Score\n",
    "    filtered_docs = []\n",
    "    search = \"No\"  # Default do not opt for web search to supplement retrieval\n",
    "    for d in documents:\n",
    "        score = chain.invoke(\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"context\": d.page_content,\n",
    "            }\n",
    "        )\n",
    "        grade = score[\"score\"]\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            search = \"Yes\"  # Perform web search\n",
    "            continue\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\n",
    "            \"documents\": filtered_docs,\n",
    "            \"question\": question,\n",
    "            \"transform_attempts\": transform_attempts\n",
    "        }\n",
    "    }\n",
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    question = state_dict[\"question\"]\n",
    "    documents = state_dict[\"documents\"]\n",
    "    transform_attempts = state_dict.get(\"transform_attempts\", 1)\n",
    "\n",
    "    # Create a prompt template with format instructions and the query\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"Du genererer spørsmål som er godt optimalisert for gjenfinning. \\n\n",
    "        Se på inndataene og prøv å resonnere om den underliggende semantiske hensikten / betydningen. \\n\n",
    "        Her er det opprinnelige spørsmålet: \\n\n",
    "        ------- \\n\n",
    "        {question} \\n\n",
    "        ------- \\n\n",
    "        Gi et forbedret spørsmål uten noen innledning, bare svar med det oppdaterte spørsmålet:  \"\"\",\n",
    "        input_variables=[\"question\"],\n",
    "    )\n",
    "\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "    # Prompt\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    better_question = chain.invoke({\"question\": question})\n",
    "\n",
    "    # Update transform_attempts in state dictionary\n",
    "    state_dict[\"transform_attempts\"] = transform_attempts + 1\n",
    "\n",
    "    return {\n",
    "        \"keys\": {\"documents\": documents, \"question\": better_question, \"transform_attempts\": transform_attempts + 1}\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, retry retrieval with a transformed query, or stop if max attempts reached.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current state of the agent, including all keys.\n",
    "\n",
    "    Returns:\n",
    "        str: Next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO GENERATE---\")\n",
    "    state_dict = state[\"keys\"]\n",
    "    filtered_documents = state_dict[\"documents\"]\n",
    "    transform_attempts = state_dict[\"transform_attempts\"]\n",
    "    transform_attempts = state_dict.get(\"transform_attempts\", 0)\n",
    "    print(state_dict)\n",
    "\n",
    "    if len(filtered_documents) == 0:\n",
    "        # No relevant documents found\n",
    "        if transform_attempts < 10:\n",
    "            # Retry retrieval with a transformed query\n",
    "            print(transform_attempts)\n",
    "            print(\"---DECISION: RETRY RETRIEVAL WITH TRANSFORMED QUERY---\")\n",
    "            return \"transform_query\"\n",
    "        else:\n",
    "            # Max attempts reached, generate answer\n",
    "            print(\"---DECISION: MAX ATTEMPTS REACHED, GENERATE ANSWER---\")\n",
    "            return \"generate\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)             # Retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # Grade documents\n",
    "workflow.add_node(\"generate\", generate)             # Generate\n",
    "workflow.add_node(\"transform_query\", transform_query)   # Transform query\n",
    "\n",
    "# Build graph\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "\n",
    "# Conditional edges based on relevance of documents\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",  # If no relevant documents found, transform query\n",
    "        \"generate\": \"generate\",                # If relevant documents found, generate answer\n",
    "    },\n",
    ")\n",
    "\n",
    "# Edge to handle retrying retrieval with transformed query\n",
    "workflow.add_edge(\"transform_query\", \"grade_documents\")\n",
    "\n",
    "# Edge for generating answer after transforming query\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files=r'/Users/adrianfolge/Documents/lokal:skole/Master/data/synthetic_data/question_with_answers.csv', split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_answers = []\n",
    "\n",
    "for i in range(50):\n",
    "    question = dataset[\"Question\"][i]\n",
    "    inputs = {\n",
    "        \"keys\": {\n",
    "            \"question\": question,\n",
    "        }\n",
    "    }\n",
    "    for output in app.stream(inputs):\n",
    "        for key, value in output.items():\n",
    "            # Node\n",
    "            pprint.pprint(f\"Node '{key}':\")\n",
    "            # Optional: print full state at each node\n",
    "            # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "        pprint.pprint(\"\\n---\\n\")\n",
    "\n",
    "    # Final generation\n",
    "    pprint.pprint(value[\"keys\"][\"generation\"])\n",
    "    list_of_answers.append(value[\"keys\"][\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(input, model):\n",
    "    return model(input)\n",
    "\n",
    "def SAS(preds, refs, model):\n",
    "    similarities = []\n",
    "    embeddings_preds = model.encode(preds)\n",
    "    embeddings_refs = model.encode(refs)\n",
    "    for i in range(len(embeddings_preds)):\n",
    "        similarity = util.pytorch_cos_sim(embeddings_preds[i], embeddings_refs[i])\n",
    "        similarities.append(similarity[0][0].item())\n",
    "    average_similarity_score = sum(similarities) / len(similarities)\n",
    "    return average_similarity_score\n",
    "\n",
    "def evaluate_predictions(references, predictions):\n",
    "    ## SAS encoder score\n",
    "    module_url = \"https://www.kaggle.com/models/google/universal-sentence-encoder/frameworks/TensorFlow2/variations/universal-sentence-encoder/versions/2\"\n",
    "    encoder_model = tensor_hub.load(module_url)\n",
    "    \n",
    "    list_of_similarity_scores = []\n",
    "    for i in range(len(predictions)):\n",
    "        similarity_score = 1-distance.cosine(embed([predictions[i]], encoder_model)[0, :],embed([references[i]], encoder_model)[0, :])\n",
    "        list_of_similarity_scores.append(similarity_score)\n",
    "    average_score = sum(list_of_similarity_scores) / len(list_of_similarity_scores)\n",
    "\n",
    "    ## SAS transformer score\n",
    "    transformer_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "\n",
    "\n",
    "    data = {\n",
    "        \"Metric\": \"Average SAS transformer Score\",\n",
    "        \"Score\":  SAS(predictions, references, transformer_model)\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = dataset[\"Answer\"]\n",
    "\n",
    "content_list = []\n",
    "\n",
    "# Choose the LLM that will drive the agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    template=\"\"\"Task: Answer Evaluation\n",
    "You are given a reference answer and a predicted answer. Your task is to determine whether the predicted answer matches the reference answer correctly. It does not have to be an exact match, but it should be somewhat the same.\n",
    "- The reference answer is the correct answer.\n",
    "- The predicted answer is the answer generated by a model or provided by a user.\n",
    "Your response should indicate whether the predicted answer is correct or not.\n",
    "Reference answer: {reference}\n",
    "Predicted answer: {prediction}\n",
    "Is the predicted answer correct? [Yes/No]\n",
    "agent_scratchpad: This is the scratchpad where you can store intermediate information.\"\"\",\n",
    "    input_variables=[\"prediction\", \"reference\"]\n",
    ")\n",
    "chain = prompt_template | llm\n",
    "\n",
    "for num in range(50):\n",
    "    score = chain.invoke(\n",
    "        {\n",
    "            \"reference\": references[num],\n",
    "            \"prediction\": list_of_answers[num],\n",
    "        }\n",
    "    )\n",
    "    content_list.append(score.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_yes = content_list.count('Yes')\n",
    "count_no = content_list.count('No')\n",
    "\n",
    "# Displaying the counts\n",
    "print(\"Number of 'Yes':\", count_yes)\n",
    "print(\"Number of 'No':\", count_no)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MASTER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
