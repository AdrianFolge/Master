{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pakker og OS key\n",
    "from corrective_rag import corrective_rag_translated\n",
    "from simple_agent_rag import simple_agent_rag, simple_agent_rag_translated\n",
    "from naive_rag import naive_rag_translated, naive_rag\n",
    "from react_rag import react_rag, react_rag_translated\n",
    "from init_vectorstore import init_vectorstore, init_semantic_vectorstore\n",
    "from evaluate_model import evaluate_model\n",
    "from ensemble_model import ensemble_models\n",
    "from semantic_model import semantic_model\n",
    "from ragas_func import ragas_with_params\n",
    "from helper_functions import create_predictions_dict, average_RAGAS_score\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "import os\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instances and file path\n",
    "instances = 100\n",
    "file_path = \"/Users/adrianfolge/Documents/lokal:skole/Master/data/synthetic_data/version_3_dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text splitter, embeddings, llm\n",
    "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "embeddings_nor = SentenceTransformerEmbeddings(model_name='sentence-transformers/paraphrase-multilingual-mpnet-base-v2')\n",
    "embeddings_trans = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init databases\n",
    "databases = init_vectorstore(embeddings_nor, text_splitter)\n",
    "databases_translated = init_vectorstore(embeddings_trans, text_splitter, translate=True)\n",
    "semantic_databases = init_semantic_vectorstore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the reference question/answers\n",
    "references = load_dataset('csv', data_files=file_path, split=f\"train[:{instances}]\")\n",
    "refs = references[\"svar\"]\n",
    "questions = references[\"spørsmål\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the answers and contexts from the models\n",
    "corrective_rag_translated_list, corrective_rag_translated_context = corrective_rag_translated(instances, file_path, databases_translated, llm)\n",
    "simple_agent_rag_list = simple_agent_rag(instances, file_path, databases, llm)\n",
    "simple_agent_rag_translated_list = simple_agent_rag_translated(instances, file_path, databases_translated, llm)\n",
    "naive_rag_list = naive_rag(instances, file_path, databases)\n",
    "naive_rag_translated_list = naive_rag_translated(instances, file_path, databases_translated)\n",
    "react_rag_translated_list,react_rag_translated_context  = react_rag_translated(instances, file_path, databases_translated, llm)\n",
    "react_rag_list, react_rag_context = react_rag(instances, file_path, databases, llm)\n",
    "semantic_rag_list, semantic_rag_context = semantic_model(instances, file_path, semantic_databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Init ensemble model\n",
    "predictions_dict = create_predictions_dict(corrective_rag_translated_list, simple_agent_rag_list, simple_agent_rag_translated_list, naive_rag_list, naive_rag_translated_list, react_rag_translated_list, react_rag_list, semantic_rag_list)\n",
    "ensembling_models_list = ensemble_models(predictions_dict, references, instances, react_rag_translated_context, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initializing RAGAS scores\n",
    "simple_agent_rag_score_RAGAS = ragas_with_params(simple_agent_rag_list, questions, simple_agent_rag_list, refs)\n",
    "corrective_rag_translated_score_RAGAS = ragas_with_params(corrective_rag_translated_list, questions, corrective_rag_translated_context, refs)\n",
    "simple_agent_translated_rag_score_RAGAS = ragas_with_params(simple_agent_rag_translated_list, questions, simple_agent_rag_translated_list, refs)\n",
    "naive_rag_score_RAGAS = ragas_with_params(naive_rag_list, questions, naive_rag_list, refs)\n",
    "naive_rag_translated_score_RAGAS = ragas_with_params(naive_rag_translated_list, questions, naive_rag_translated_list, refs)\n",
    "react_rag_translated_score_RAGAS = ragas_with_params(react_rag_translated_list, questions, react_rag_translated_context, refs)\n",
    "react_rag_score_RAGAS = ragas_with_params(react_rag_list, questions, react_rag_context, refs)\n",
    "semantic_rag_score_RAGAS = ragas_with_params(semantic_rag_list, questions, semantic_rag_context, refs)\n",
    "ensemble_models_score_RAGAS = ragas_with_params(ensembling_models_list, questions, react_rag_translated_context, refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the RAGAS scores\n",
    "print(simple_agent_rag_score_RAGAS)\n",
    "avg_simple_agent_rag_score_RAGAS = average_RAGAS_score(simple_agent_rag_score_RAGAS)\n",
    "print(corrective_rag_translated_score_RAGAS) \n",
    "avg_corrective_rag_translated_score_RAGAS = average_RAGAS_score(corrective_rag_translated_score_RAGAS)\n",
    "print(simple_agent_translated_rag_score_RAGAS)\n",
    "avg_simple_agent_translated_rag_score_RAGAS = average_RAGAS_score(simple_agent_translated_rag_score_RAGAS)\n",
    "print(naive_rag_score_RAGAS) \n",
    "avg_naive_rag_score_RAGAS = average_RAGAS_score(naive_rag_score_RAGAS)\n",
    "print(naive_rag_translated_score_RAGAS) \n",
    "avg_naive_rag_translated_score_RAGAS = average_RAGAS_score(naive_rag_translated_score_RAGAS)\n",
    "print(react_rag_translated_score_RAGAS) \n",
    "avg_react_rag_translated_score_RAGAS = average_RAGAS_score(react_rag_translated_score_RAGAS)\n",
    "print(react_rag_score_RAGAS)\n",
    "avg_react_rag_score_RAGAS = average_RAGAS_score(react_rag_score_RAGAS)\n",
    "print(semantic_rag_score_RAGAS) \n",
    "avg_semantic_rag_score_RAGAS = average_RAGAS_score(semantic_rag_score_RAGAS)\n",
    "print(ensemble_models_score_RAGAS)\n",
    "avg_ensemble_models_score_RAGAS = average_RAGAS_score(ensemble_models_score_RAGAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the yes/no evals\n",
    "print(\"Scores for corrective RAG\")\n",
    "corrective_rag_score = evaluate_model(corrective_rag_translated_list, refs, instances)\n",
    "print(\"#############\")\n",
    "print(\"Scores for simple agent\")\n",
    "simple_agent_score = evaluate_model(simple_agent_rag_list, refs, instances)\n",
    "print(\"#############\")\n",
    "print(\"Scores for simple agent translated\")\n",
    "simple_agent_translated_score = evaluate_model(simple_agent_rag_translated_list, refs, instances)\n",
    "print(\"#############\")\n",
    "print(\"Scores for naive rag\")\n",
    "naive_rag_score = evaluate_model(naive_rag_list, refs, instances)\n",
    "print(\"#############\")\n",
    "print(\"Scores for naive rag translated\")\n",
    "naive_rag_translated_score = evaluate_model(naive_rag_translated_list, refs, instances)\n",
    "print(\"#############\")\n",
    "print(\"Scores for react RAG\")\n",
    "react_rag_score = evaluate_model(react_rag_list, refs, instances)\n",
    "print(\"#############\")\n",
    "print(\"Scores for react RAG translated\")\n",
    "react_rag_translated_score = evaluate_model(react_rag_translated_list, refs, instances)\n",
    "print(\"#############\")\n",
    "print(\"Scores for semantic RAG\")\n",
    "semantic_rag_score = evaluate_model(semantic_rag_list, refs, instances)\n",
    "print(\"#############\")\n",
    "print(\"Scores for ensembling models\")\n",
    "ensemble_models_score = evaluate_model(ensembling_models_list, refs, instances)\n",
    "print(\"#############\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Getting the answer relevancy for all the models:\n",
    "answer_relevancy_ensemble = (ensemble_models_score_RAGAS[\"answer_relevancy\"])\n",
    "answer_relevancy_corrective =(corrective_rag_translated_score_RAGAS[\"answer_relevancy\"])\n",
    "answer_relevancy_simple_agent_translated = (simple_agent_translated_rag_score_RAGAS[\"answer_relevancy\"])\n",
    "answer_relevancy_simple_agent = (simple_agent_rag_score_RAGAS[\"answer_relevancy\"])\n",
    "answer_relevancy_naive_rag = (naive_rag_score_RAGAS[\"answer_relevancy\"])\n",
    "answer_relevance_naive_rag_translated = (naive_rag_translated_score_RAGAS[\"answer_relevancy\"])\n",
    "answer_relevancy_react = (react_rag_score_RAGAS[\"answer_relevancy\"])\n",
    "answer_relevancy_react_translated = (react_rag_translated_score_RAGAS[\"answer_relevancy\"])\n",
    "answer_relevancy_semantic = (semantic_rag_score_RAGAS[\"answer_relevancy\"])\n",
    "# Scores for each model (example data)\n",
    "model_names = [\"Corrective RAG\", \"Simple agent\", \"Simple agent translated\", \"Naive rag\", \"Naive rag translated\", \"React rag\", \"React rag translated\", \"Semantic rag\",\"Model ensembling\"]\n",
    "scores = [answer_relevancy_corrective, answer_relevancy_simple_agent, answer_relevancy_simple_agent_translated, answer_relevancy_naive_rag, answer_relevance_naive_rag_translated, answer_relevancy_react, answer_relevancy_react_translated, answer_relevancy_semantic,answer_relevancy_ensemble]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, scores, color='skyblue')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Answer relevancy scores for the models')\n",
    "plt.ylim(0.5, 1)  # Set y-axis limits to 0 and 1\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg RAGAS Scores for each model (example data)\n",
    "model_names = [\"Corrective RAG\", \"Simple agent\", \"Simple agent translated\", \"Naive rag\", \"Naive rag translated\", \"React rag\", \"React rag translated\", \"Semantic rag\",\"Model ensembling\"]\n",
    "scores = [avg_corrective_rag_translated_score_RAGAS, avg_simple_agent_rag_score_RAGAS, avg_simple_agent_translated_rag_score_RAGAS, avg_naive_rag_score_RAGAS, avg_naive_rag_translated_score_RAGAS, avg_react_rag_score_RAGAS, avg_react_rag_translated_score_RAGAS, avg_semantic_rag_score_RAGAS,avg_ensemble_models_score_RAGAS]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, scores, color='skyblue')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Average RAGAS scores for the models')\n",
    "plt.ylim(0.8, 1)  # Set y-axis limits to 0 and 1\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct Scores for each model (example data)\n",
    "model_names = [\"Corrective RAG\", \"Simple agent\", \"Simple agent translated\", \"Naive rag\", \"Naive rag translated\", \"React rag\", \"React rag translated\", \"Semantic rag\",\"Model ensembling\"]\n",
    "scores = [corrective_rag_score, simple_agent_score, simple_agent_translated_score, naive_rag_score, naive_rag_translated_score, react_rag_score, react_rag_translated_score, semantic_rag_score,ensemble_models_score]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(model_names, scores, color='skyblue')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Correct/Not correct Scores of Different Models')\n",
    "plt.ylim(0, 1)  # Set y-axis limits to 0 and 1\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Skriver alle resultatene til en tekst-fil\n",
    "import math\n",
    "\n",
    "\n",
    "answer_relevancy_scores = [answer_relevancy_corrective, answer_relevancy_simple_agent, answer_relevancy_simple_agent_translated, answer_relevancy_naive_rag, answer_relevance_naive_rag_translated, answer_relevancy_react, answer_relevancy_react_translated, answer_relevancy_semantic,answer_relevancy_ensemble]\n",
    "average_ragas_scores = [avg_corrective_rag_translated_score_RAGAS, avg_simple_agent_rag_score_RAGAS, avg_simple_agent_translated_rag_score_RAGAS, avg_naive_rag_score_RAGAS, avg_naive_rag_translated_score_RAGAS, avg_react_rag_score_RAGAS, avg_react_rag_translated_score_RAGAS, avg_semantic_rag_score_RAGAS,avg_ensemble_models_score_RAGAS]\n",
    "correct_not_scores = [corrective_rag_score, simple_agent_score, simple_agent_translated_score, naive_rag_score, naive_rag_translated_score, react_rag_score, react_rag_translated_score, semantic_rag_score,ensemble_models_score]\n",
    "# Open a text file in write mode\n",
    "with open('/Users/adrianfolge/Documents/lokal:skole/Master/master_folder/eval_data/final_eval_data.txt', 'w') as file:\n",
    "    # Write the elements of list1 to the file\n",
    "    file.write(\"Answer relevancy scores:\\n\")\n",
    "    for item in answer_relevancy_scores:\n",
    "        file.write(str(item) + '\\n')\n",
    "    \n",
    "    # Write a separator\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    # Write the elements of list2 to the file\n",
    "    file.write(\"Average ragas scores:\\n\")\n",
    "    for item in average_ragas_scores:\n",
    "        if math.isnan(item):\n",
    "            file.write(\"0\\n\")\n",
    "        else:\n",
    "            file.write(str(item) + '\\n')\n",
    "    \n",
    "    # Write a separator\n",
    "    file.write(\"\\n\")\n",
    "    \n",
    "    # Write the elements of list3 to the file\n",
    "    file.write(\"Correct not correct scores:\\n\")\n",
    "    for item in correct_not_scores:\n",
    "        file.write(str(item) + '\\n')\n",
    "    \n",
    "print(\"Lists have been saved to 'lists.txt' file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MASTER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
